
# Trade Management System - Rust

## Overview

Welcome to the Trade Management System project, built using Rust. This system provides a robust and efficient solution for tracking, managing, and analyzing trades. Traders can calculate their profit and loss over time through this system's functionalities. The project offers a RESTful API, enabling users to execute trades, update trade details, retrieve trade information, and perform advanced tasks.

## Installation and Configuration

To quickly set up and configure the project on your Linux or macOS system, you can use the following command in your terminal:

    curl --proto '=https' --tlsv1.2 -sSf https://raw.githubusercontent.com/dalmasjunior/TradeManagementSystem/main/install.sh | sh

This command will download and execute an installation script, wich will handle the necessary setup steps for you.

## Manual Installation

If you prefer manual installation, follow these steps:

1. First, you will need to clone the repository:

    git clone https://github.com/dalmasjunior/TradeManagementSystem.git

2. Change into the new directory:

    cd TradeManagementSystem

3. Install diesel-cli:

    cargo  install  diesel_cli  --no-default-features  --features  sqlite

4. To compile the project, use the following command:

    cargo build

5. Configure the SQLite database:

    diesel setup

6. Run migrations:

    diesel migration run

## Run the Project

1. Now you are ready to run the application with this command:
    bash cargo run

2. By default, the server will start on port 9000. Open your browser or Postman, and visit:
    http://localhost:9000

## Viewing API Documentation

To explore the detailed API documentation generated by Rust, you can use the following command:

    cargo doc --open

Running this command will generate the documentation and automatically open it in your default web browser. This documentation provides comprehensive information about the project's internal components, functions, and modules.

## Testing

A suite of unit and integration tests have been set up for validating the functionality of the API and the accuracy of trade calculations. You can run these tests using:

     cargo test

## JWT Authentication and Security Considerations

This project employs JSON Web Tokens (JWT) for secure authentication and authorization. JWTs are used to verify the identity of users and ensure that only authorized users can access and manipulate trade data. JWTs offer several advantages, including being stateless, decentralized, and customizable. However, there are potential vulnerabilities that need to be addressed:

### Vulnerabilities and Mitigations

- **Token Leakage**: Ensure that JWT secrets are stored securely and not exposed in version control or other public places.
- **Token Substitution**: Use strong encryption algorithms and keep your secret keys private to prevent attackers from generating valid tokens.
- **Expired Tokens**: Set appropriate expiration times for tokens and handle token expiration gracefully in your code.
- **Data Integrity**: Use HTTPS to prevent interception and tampering of JWTs during transmission.

## Comparison with Other Approaches

JWT is a widely adopted approach for securing APIs due to its simplicity, scalability, and cross-service compatibility. Compared to traditional session-based authentication, JWTs eliminate the need for server-side storage of session data. Additionally, JWTs can be used effectively in microservices architectures. While there are other authentication methods like OAuth and API keys, JWTs offer a balance between security and ease of implementation.

## Optimization Approaches for Large Trade Data

Consider implementing the following optimization approaches as the trade data grows :

- **Database Indexing**: Ensure proper indexing of frequently used columns like user_id, created_at, and asset to speed up data retrieval.

- **Caching**: Cache frequently computed values, such as cumulative fees or slippage calculations, to reduce redundant calculations.

- **Batch Processing**: Divide data into batches for calculations to manage memory and processing load effectively.

- **Parallel Processing**: Use async/await or libraries like Rayon to perform calculations concurrently and utilize multi-core processors.

- **Data Preprocessing**: Generate summary tables or aggregated data to reduce on-the-fly calculations.

- **Database Aggregation**: Utilize SQL aggregation functions for calculations directly within the database.

- **Data Warehousing**: Consider moving historical data to data warehouses optimized for analytics.

- **Profiling and Monitoring**: Regularly profile and monitor code to identify bottlenecks and memory usage patterns.

- **Horizontal Scaling**: Scale horizontally by adding more servers or containers to handle increased load.

## Contributing

We would love for you to contribute to the project. Please feel free to open pull requests or issues.
